{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/pplateena/MMPAI-NULP/blob/main/l4/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "id": "f59965612c9df2d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Lab 4: Neural Network Implementation with PyTorch\n# Multi-Layer Perceptron for Titanic Survival Prediction\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"üö¢ Lab 4: Neural Network Implementation with PyTorch\")\nprint(\"=\"*60)\nprint(\"Building MLP for Titanic Survival Prediction\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "id": "62hq9shwtyx",
   "source": "# Task 1: Data Loading and Preparation\nprint(\"=\"*50)\nprint(\"TASK 1: DATA LOADING AND PREPARATION\")\nprint(\"=\"*50)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üîß Using device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\n# Load dataset from Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Load the preprocessed dataset from Lab 2\ndrive_path = '/content/drive/MyDrive/transformed_df.csv'\ndf = pd.read_csv(drive_path)\n\nprint(f\"‚úÖ Dataset loaded: {df.shape}\")\nprint(f\"üìã Columns: {list(df.columns)}\")\nprint(f\"üéØ Target distribution: {df['Survived'].value_counts().to_dict()}\")\n\n# Separate features and target\nX = df.drop('Survived', axis=1)\ny = df['Survived']\n\nprint(f\"\\nüìä After preprocessing:\")\nprint(f\"   Features shape: {X.shape}\")\nprint(f\"   Target shape: {y.shape}\")\nprint(f\"   Feature names: {list(X.columns)}\")\nprint(f\"   Survival rate: {y.mean():.3f}\")\n\nprint(\"\\n‚úÖ Task 1 completed: Data loading and preparation finished\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "azaayfpdny",
   "source": "# Task 2: Data Splitting and Scaling\nprint(\"=\"*50)\nprint(\"TASK 2: DATA SPLITTING AND SCALING\")\nprint(\"=\"*50)\n\n# Convert to numpy arrays\nX_np = X.values.astype(np.float32)\ny_np = y.values.astype(np.float32)\n\n# Split the data: 60% train, 20% validation, 20% test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2\n)\n\nprint(f\"üìä Data split:\")\nprint(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_np)*100:.1f}%)\")\nprint(f\"   Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_np)*100:.1f}%)\")\nprint(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_np)*100:.1f}%)\")\n\n# Check class distribution\nprint(f\"\\nüéØ Class distribution:\")\nprint(f\"   Train: {np.bincount(y_train.astype(int))} (Survival rate: {y_train.mean():.3f})\")\nprint(f\"   Val:   {np.bincount(y_val.astype(int))} (Survival rate: {y_val.mean():.3f})\")\nprint(f\"   Test:  {np.bincount(y_test.astype(int))} (Survival rate: {y_test.mean():.3f})\")\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\nüìè Feature scaling completed:\")\nprint(f\"   Original range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\nprint(f\"   Scaled range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_scaled)\nX_val_tensor = torch.FloatTensor(X_val_scaled)\nX_test_tensor = torch.FloatTensor(X_test_scaled)\n\ny_train_tensor = torch.FloatTensor(y_train)\ny_val_tensor = torch.FloatTensor(y_val)\ny_test_tensor = torch.FloatTensor(y_test)\n\nprint(f\"\\nüî• Converted to PyTorch tensors:\")\nprint(f\"   X_train: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}\")\nprint(f\"   y_train: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}\")\n\n# Create DataLoaders\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\nüì¶ DataLoaders created:\")\nprint(f\"   Batch size: {batch_size}\")\nprint(f\"   Training batches: {len(train_loader)}\")\nprint(f\"   Validation batches: {len(val_loader)}\")\nprint(f\"   Test batches: {len(test_loader)}\")\n\nprint(\"\\n‚úÖ Task 2 completed: Data splitting and scaling finished\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "rly1uitck6",
   "source": "# Task 3: MLP Architecture Definition\nprint(\"=\"*50)\nprint(\"TASK 3: MLP ARCHITECTURE DEFINITION\")\nprint(\"=\"*50)\n\nclass TitanicMLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron for Titanic Survival Prediction\n    \n    Architecture:\n    - Input layer: num_features\n    - Hidden layer 1: 64 neurons + ReLU + Dropout\n    - Hidden layer 2: 32 neurons + ReLU + Dropout  \n    - Hidden layer 3: 16 neurons + ReLU + Dropout\n    - Output layer: 1 neuron + Sigmoid (for binary classification)\n    \"\"\"\n    \n    def __init__(self, num_features, dropout_rate=0.3):\n        super(TitanicMLP, self).__init__()\n        \n        self.network = nn.Sequential(\n            # Input layer\n            nn.Linear(num_features, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Hidden layer 1\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Hidden layer 2  \n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Output layer\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n        \n        # Initialize weights\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                nn.init.zeros_(module.bias)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Get number of features\nnum_features = X_train_tensor.shape[1]\n\n# Create model instance\nmodel = TitanicMLP(num_features=num_features, dropout_rate=0.3)\nmodel = model.to(device)\n\nprint(f\"üß† MLP Architecture:\")\nprint(f\"   Input features: {num_features}\")\nprint(f\"   Architecture: {num_features} ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1\")\nprint(f\"   Activation: ReLU (hidden layers), Sigmoid (output)\")\nprint(f\"   Dropout rate: 0.3\")\nprint(f\"   Weight initialization: Xavier Uniform\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\nüìä Model Parameters:\")\nprint(f\"   Total parameters: {total_params:,}\")\nprint(f\"   Trainable parameters: {trainable_params:,}\")\n\n# Print model architecture\nprint(f\"\\nüèóÔ∏è Detailed Architecture:\")\nprint(model)\n\n# Model summary function\ndef model_summary(model, input_size):\n    \"\"\"Print model summary similar to Keras\"\"\"\n    print(f\"\\nüìã Model Summary:\")\n    print(\"-\" * 65)\n    print(f\"{'Layer':<15} {'Input Shape':<15} {'Output Shape':<15} {'Param #':<10}\")\n    print(\"-\" * 65)\n    \n    total_params = 0\n    x = torch.randn(1, input_size)\n    \n    for name, layer in model.named_children():\n        if hasattr(layer, '__iter__'):\n            for i, sublayer in enumerate(layer):\n                if isinstance(sublayer, nn.Linear):\n                    in_features = sublayer.in_features\n                    out_features = sublayer.out_features\n                    params = in_features * out_features + out_features\n                    total_params += params\n                    \n                    print(f\"Linear_{i+1}      ({in_features:,})          ({out_features:,})          {params:,}\")\n                elif isinstance(sublayer, (nn.ReLU, nn.Dropout, nn.Sigmoid)):\n                    layer_name = sublayer.__class__.__name__\n                    if isinstance(sublayer, nn.Dropout):\n                        layer_name += f\"({sublayer.p})\"\n                    print(f\"{layer_name:<15} {'':<15} {'':<15} {'0':<10}\")\n    \n    print(\"-\" * 65)\n    print(f\"Total params: {total_params:,}\")\n    print(\"-\" * 65)\n\nmodel_summary(model, num_features)\n\nprint(\"\\n‚úÖ Task 3 completed: MLP architecture defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3bn5kzt6hhh",
   "source": "# Task 4: Training Configuration\nprint(\"=\"*50)\nprint(\"TASK 4: TRAINING CONFIGURATION\")\nprint(\"=\"*50)\n\n# Define loss function and optimizer\ncriterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)\n\n# Training parameters\nnum_epochs = 100\nearly_stopping_patience = 15\nbest_val_loss = float('inf')\nepochs_without_improvement = 0\n\nprint(f\"‚öôÔ∏è Training Configuration:\")\nprint(f\"   Loss function: Binary Cross-Entropy (BCELoss)\")\nprint(f\"   Optimizer: Adam\")\nprint(f\"   Learning rate: 0.001\")\nprint(f\"   Weight decay: 1e-5\")\nprint(f\"   LR Scheduler: StepLR (step_size=30, gamma=0.7)\")\nprint(f\"   Number of epochs: {num_epochs}\")\nprint(f\"   Early stopping patience: {early_stopping_patience}\")\n\n# Metrics calculation functions\ndef calculate_metrics(y_true, y_pred, y_prob):\n    \"\"\"Calculate classification metrics\"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    auc = roc_auc_score(y_true, y_prob)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc\n    }\n\ndef evaluate_model(model, dataloader, criterion, device):\n    \"\"\"Evaluate model on given dataloader\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_predictions = []\n    all_probabilities = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs.squeeze(), targets)\n            \n            total_loss += loss.item()\n            \n            # Convert to numpy for metrics calculation\n            probabilities = outputs.squeeze().cpu().numpy()\n            predictions = (probabilities > 0.5).astype(int)\n            targets_np = targets.cpu().numpy()\n            \n            all_predictions.extend(predictions)\n            all_probabilities.extend(probabilities)\n            all_targets.extend(targets_np)\n    \n    avg_loss = total_loss / len(dataloader)\n    metrics = calculate_metrics(all_targets, all_predictions, all_probabilities)\n    \n    return avg_loss, metrics\n\n# Initialize tracking lists\ntrain_losses = []\nval_losses = []\ntrain_metrics_history = []\nval_metrics_history = []\n\nprint(f\"\\nüìä Metrics to track:\")\nprint(f\"   - Loss (Training & Validation)\")\nprint(f\"   - Accuracy, Precision, Recall, F1-Score, AUC\")\n\nprint(\"\\n‚úÖ Task 4 completed: Training configuration set up\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p7lifza9fzm",
   "source": "# Task 5: Model Training\nprint(\"=\"*50)\nprint(\"TASK 5: MODEL TRAINING\")\nprint(\"=\"*50)\n\nprint(f\"üöÄ Starting training for {num_epochs} epochs...\")\nprint(f\"üîß Device: {device}\")\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    \n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), targets)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    # Calculate average training loss\n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    \n    # Evaluation phase\n    val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device)\n    train_loss_eval, train_metrics = evaluate_model(model, train_loader, criterion, device)\n    \n    val_losses.append(val_loss)\n    train_metrics_history.append(train_metrics)\n    val_metrics_history.append(val_metrics)\n    \n    # Learning rate scheduling\n    scheduler.step()\n    \n    # Early stopping check\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_without_improvement = 0\n        # Save best model\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        epochs_without_improvement += 1\n    \n    # Print progress every 10 epochs\n    if (epoch + 1) % 10 == 0 or epoch == 0:\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | \"\n              f\"Train Loss: {avg_train_loss:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} | \"\n              f\"Val Acc: {val_metrics['accuracy']:.4f} | \"\n              f\"Val F1: {val_metrics['f1']:.4f} | \"\n              f\"LR: {current_lr:.6f}\")\n    \n    # Early stopping\n    if epochs_without_improvement >= early_stopping_patience:\n        print(f\"\\n‚è∞ Early stopping triggered after {epoch+1} epochs\")\n        print(f\"   Best validation loss: {best_val_loss:.4f}\")\n        break\n\n# Load best model\nmodel.load_state_dict(torch.load('best_model.pth'))\n\nprint(f\"\\nüéâ Training completed!\")\nprint(f\"   Total epochs: {epoch+1}\")\nprint(f\"   Best validation loss: {best_val_loss:.4f}\")\n\n# Final evaluation on all sets\nprint(f\"\\nüìä Final Model Evaluation:\")\ntrain_loss_final, train_metrics_final = evaluate_model(model, train_loader, criterion, device)\nval_loss_final, val_metrics_final = evaluate_model(model, val_loader, criterion, device)\n\nprint(f\"\\nüìà Training Set Performance:\")\nfor metric, value in train_metrics_final.items():\n    print(f\"   {metric.capitalize()}: {value:.4f}\")\n\nprint(f\"\\nüìà Validation Set Performance:\")\nfor metric, value in val_metrics_final.items():\n    print(f\"   {metric.capitalize()}: {value:.4f}\")\n\nprint(\"\\n‚úÖ Task 5 completed: Model training finished\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ljg1fv93w9b",
   "source": "# Task 6: Learning Curves and Visualization\nprint(\"=\"*50)\nprint(\"TASK 6: LEARNING CURVES AND VISUALIZATION\")\nprint(\"=\"*50)\n\n# Create comprehensive visualizations\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Neural Network Training Analysis - Lab 4', fontsize=16, fontweight='bold')\n\n# 1. Loss curves\nepochs_range = range(1, len(train_losses) + 1)\naxes[0, 0].plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\naxes[0, 0].plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\naxes[0, 0].set_title('Training vs Validation Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Find overfitting point\nmin_val_loss_idx = np.argmin(val_losses)\naxes[0, 0].axvline(x=min_val_loss_idx + 1, color='green', linestyle='--', alpha=0.7, \n                   label=f'Best Val Loss (Epoch {min_val_loss_idx + 1})')\naxes[0, 0].legend()\n\n# 2. Accuracy curves\ntrain_accuracies = [metrics['accuracy'] for metrics in train_metrics_history]\nval_accuracies = [metrics['accuracy'] for metrics in val_metrics_history]\n\naxes[0, 1].plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\naxes[0, 1].plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\naxes[0, 1].set_title('Training vs Validation Accuracy')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Accuracy')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. F1-Score curves\ntrain_f1_scores = [metrics['f1'] for metrics in train_metrics_history]\nval_f1_scores = [metrics['f1'] for metrics in val_metrics_history]\n\naxes[0, 2].plot(epochs_range, train_f1_scores, 'b-', label='Training F1', linewidth=2)\naxes[0, 2].plot(epochs_range, val_f1_scores, 'r-', label='Validation F1', linewidth=2)\naxes[0, 2].set_title('Training vs Validation F1-Score')\naxes[0, 2].set_xlabel('Epoch')\naxes[0, 2].set_ylabel('F1-Score')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. AUC curves\ntrain_aucs = [metrics['auc'] for metrics in train_metrics_history]\nval_aucs = [metrics['auc'] for metrics in val_metrics_history]\n\naxes[1, 0].plot(epochs_range, train_aucs, 'b-', label='Training AUC', linewidth=2)\naxes[1, 0].plot(epochs_range, val_aucs, 'r-', label='Validation AUC', linewidth=2)\naxes[1, 0].set_title('Training vs Validation AUC')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('AUC')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. Precision and Recall curves\ntrain_precisions = [metrics['precision'] for metrics in train_metrics_history]\ntrain_recalls = [metrics['recall'] for metrics in train_metrics_history]\nval_precisions = [metrics['precision'] for metrics in val_metrics_history]\nval_recalls = [metrics['recall'] for metrics in val_metrics_history]\n\naxes[1, 1].plot(epochs_range, train_precisions, 'b-', label='Train Precision', linewidth=2)\naxes[1, 1].plot(epochs_range, val_precisions, 'r-', label='Val Precision', linewidth=2)\naxes[1, 1].plot(epochs_range, train_recalls, 'b--', label='Train Recall', linewidth=2)\naxes[1, 1].plot(epochs_range, val_recalls, 'r--', label='Val Recall', linewidth=2)\naxes[1, 1].set_title('Precision and Recall Curves')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Score')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# 6. Overfitting analysis\ngeneralization_gap = np.array(train_accuracies) - np.array(val_accuracies)\naxes[1, 2].plot(epochs_range, generalization_gap, 'purple', linewidth=2)\naxes[1, 2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\naxes[1, 2].set_title('Generalization Gap (Train - Val Accuracy)')\naxes[1, 2].set_xlabel('Epoch')\naxes[1, 2].set_ylabel('Accuracy Gap')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Add horizontal line at gap threshold\ngap_threshold = 0.05\naxes[1, 2].axhline(y=gap_threshold, color='red', linestyle='--', alpha=0.7, \n                   label=f'Overfitting threshold ({gap_threshold})')\naxes[1, 2].axhline(y=-gap_threshold, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analysis summary\nprint(f\"üìä Learning Curve Analysis:\")\nprint(f\"-\" * 50)\n\n# Overfitting detection\nfinal_gap = generalization_gap[-1]\nmax_gap = np.max(generalization_gap)\nmin_val_loss_epoch = min_val_loss_idx + 1\ntotal_epochs_trained = len(train_losses)\n\nprint(f\"üîç Overfitting Analysis:\")\nprint(f\"   Final generalization gap: {final_gap:.4f}\")\nprint(f\"   Maximum generalization gap: {max_gap:.4f}\")\nprint(f\"   Best validation loss at epoch: {min_val_loss_epoch}\")\nprint(f\"   Total epochs trained: {total_epochs_trained}\")\n\nif final_gap > 0.05:\n    overfitting_status = \"‚ö†Ô∏è Moderate overfitting detected\"\nelif final_gap > 0.1:\n    overfitting_status = \"üö® Significant overfitting detected\"\nelse:\n    overfitting_status = \"‚úÖ Good generalization\"\n\nprint(f\"   Status: {overfitting_status}\")\n\n# Performance trends\nval_acc_trend = \"improving\" if val_accuracies[-1] > val_accuracies[max(0, len(val_accuracies)-10)] else \"declining\"\nval_loss_trend = \"improving\" if val_losses[-1] < val_losses[max(0, len(val_losses)-10)] else \"increasing\"\n\nprint(f\"\\nüìà Performance Trends (last 10 epochs):\")\nprint(f\"   Validation accuracy: {val_acc_trend}\")\nprint(f\"   Validation loss: {val_loss_trend}\")\n\n# Best performance summary\nbest_val_acc = max(val_accuracies)\nbest_val_f1 = max(val_f1_scores)\nbest_val_auc = max(val_aucs)\n\nprint(f\"\\nüèÜ Best Performance Achieved:\")\nprint(f\"   Best validation accuracy: {best_val_acc:.4f} (epoch {val_accuracies.index(best_val_acc)+1})\")\nprint(f\"   Best validation F1-score: {best_val_f1:.4f} (epoch {val_f1_scores.index(best_val_f1)+1})\")\nprint(f\"   Best validation AUC: {best_val_auc:.4f} (epoch {val_aucs.index(best_val_auc)+1})\")\n\nprint(\"\\n‚úÖ Task 6 completed: Learning curves analysis finished\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qsal6cnu20s",
   "source": "# Task 7: Final Model Evaluation and Test Results\nprint(\"=\"*50)\nprint(\"TASK 7: FINAL MODEL EVALUATION\")\nprint(\"=\"*50)\n\n# Evaluate on test set\nprint(\"üß™ Final evaluation on test set...\")\ntest_loss, test_metrics = evaluate_model(model, test_loader, criterion, device)\n\nprint(f\"\\nüìä Test Set Performance:\")\nprint(f\"   Loss: {test_loss:.4f}\")\nfor metric, value in test_metrics.items():\n    print(f\"   {metric.capitalize()}: {value:.4f}\")\n\n# Get predictions for confusion matrix\nmodel.eval()\ntest_predictions = []\ntest_targets = []\ntest_probabilities = []\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        \n        probabilities = outputs.squeeze().cpu().numpy()\n        predictions = (probabilities > 0.5).astype(int)\n        \n        test_predictions.extend(predictions)\n        test_probabilities.extend(probabilities)\n        test_targets.extend(targets.cpu().numpy())\n\n# Create confusion matrix\ncm = confusion_matrix(test_targets, test_predictions)\n\n# Visualization of final results\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfig.suptitle('Final Model Evaluation - Test Set Results', fontsize=14, fontweight='bold')\n\n# 1. Confusion Matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\naxes[0].set_title('Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_xticklabels(['Died', 'Survived'])\naxes[0].set_yticklabels(['Died', 'Survived'])\n\n# 2. Prediction Probability Distribution\naxes[1].hist([p for p, t in zip(test_probabilities, test_targets) if t == 0], \n             bins=20, alpha=0.7, label='Died', color='red')\naxes[1].hist([p for p, t in zip(test_probabilities, test_targets) if t == 1], \n             bins=20, alpha=0.7, label='Survived', color='blue')\naxes[1].set_title('Prediction Probability Distribution')\naxes[1].set_xlabel('Predicted Probability')\naxes[1].set_ylabel('Count')\naxes[1].legend()\naxes[1].axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')\n\n# 3. Performance Comparison\nmodels = ['Neural Network']\naccuracies = [test_metrics['accuracy']]\nf1_scores = [test_metrics['f1']]\naucs = [test_metrics['auc']]\n\nx = np.arange(len(models))\nwidth = 0.25\n\naxes[2].bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\naxes[2].bar(x, f1_scores, width, label='F1-Score', alpha=0.8)\naxes[2].bar(x + width, aucs, width, label='AUC', alpha=0.8)\n\naxes[2].set_title('Model Performance Metrics')\naxes[2].set_ylabel('Score')\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(models)\naxes[2].legend()\naxes[2].set_ylim(0, 1)\n\n# Add value labels on bars\nfor i, (acc, f1, auc) in enumerate(zip(accuracies, f1_scores, aucs)):\n    axes[2].text(i - width, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n    axes[2].text(i, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom')\n    axes[2].text(i + width, auc + 0.01, f'{auc:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Detailed classification report\nfrom sklearn.metrics import classification_report\n\nprint(f\"\\nüìã Detailed Classification Report:\")\nprint(\"-\" * 60)\nprint(classification_report(test_targets, test_predictions, \n                          target_names=['Died', 'Survived'], digits=4))\n\n# Model analysis summary\nprint(f\"\\nüìä Model Analysis Summary:\")\nprint(\"=\" * 60)\n\nprint(f\"\\nüß† Architecture Summary:\")\nprint(f\"   Model type: Multi-Layer Perceptron (MLP)\")\nprint(f\"   Input features: {num_features}\")\nprint(f\"   Hidden layers: 3 layers (64 ‚Üí 32 ‚Üí 16)\")\nprint(f\"   Activation functions: ReLU (hidden), Sigmoid (output)\")\nprint(f\"   Regularization: Dropout (0.3), Weight decay (1e-5)\")\nprint(f\"   Total parameters: {total_params:,}\")\n\nprint(f\"\\n‚öôÔ∏è Training Configuration:\")\nprint(f\"   Optimizer: Adam (lr=0.001)\")\nprint(f\"   Loss function: Binary Cross-Entropy\")\nprint(f\"   Batch size: {batch_size}\")\nprint(f\"   Epochs trained: {len(train_losses)}\")\nprint(f\"   Early stopping: {early_stopping_patience} epochs patience\")\n\nprint(f\"\\nüéØ Final Performance:\")\nprint(f\"   Test Accuracy: {test_metrics['accuracy']:.4f}\")\nprint(f\"   Test Precision: {test_metrics['precision']:.4f}\")\nprint(f\"   Test Recall: {test_metrics['recall']:.4f}\")\nprint(f\"   Test F1-Score: {test_metrics['f1']:.4f}\")\nprint(f\"   Test AUC: {test_metrics['auc']:.4f}\")\n\nprint(f\"\\nüîç Generalization Analysis:\")\nfinal_train_acc = train_metrics_final['accuracy']\nfinal_val_acc = val_metrics_final['accuracy']\nfinal_test_acc = test_metrics['accuracy']\n\nprint(f\"   Training accuracy: {final_train_acc:.4f}\")\nprint(f\"   Validation accuracy: {final_val_acc:.4f}\")\nprint(f\"   Test accuracy: {final_test_acc:.4f}\")\nprint(f\"   Train-Val gap: {final_train_acc - final_val_acc:.4f}\")\nprint(f\"   Val-Test gap: {final_val_acc - final_test_acc:.4f}\")\n\nif abs(final_val_acc - final_test_acc) < 0.02:\n    generalization_quality = \"‚úÖ Excellent generalization\"\nelif abs(final_val_acc - final_test_acc) < 0.05:\n    generalization_quality = \"‚úÖ Good generalization\"\nelse:\n    generalization_quality = \"‚ö†Ô∏è Potential generalization issues\"\n\nprint(f\"   Generalization quality: {generalization_quality}\")\n\nprint(f\"\\nüèÅ Lab 4 Summary:\")\nprint(f\"   ‚úÖ Successfully implemented MLP with PyTorch\")\nprint(f\"   ‚úÖ Achieved {test_metrics['accuracy']:.1%} test accuracy\")\nprint(f\"   ‚úÖ Implemented proper training pipeline with early stopping\")\nprint(f\"   ‚úÖ Generated comprehensive learning curves analysis\")\nprint(f\"   ‚úÖ Demonstrated understanding of overfitting detection\")\n\nprint(\"\\n‚úÖ Task 7 completed: Final evaluation and summary report finished\")\nprint(\"\\nüéâ Lab 4: Neural Network Implementation - COMPLETED! üéâ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
